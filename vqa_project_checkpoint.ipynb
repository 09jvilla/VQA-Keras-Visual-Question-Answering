{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS230 Project\n",
    "# Deep Learning for VQA: Visual Question Answering\n",
    "\n",
    "Stephanie Do <br> Alona King <br> Jennifer Villa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project explores the challenge of visual question answering (VQA) -- given an image and an open ended question concerning the image, build a model that returns a correct answer. This topic requires synthesizing both visual and language modalities, and combining the two to produce a natural language answer, making it more challenging than traditional image classification. VQA challenges researchers to create networks with a more sophisticated level of understanding that could ultimately be used to help robots or drones navigate their environment. These networks could also give visually impaired people a more rich description of a scene, or be used for better image or product search within a database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "For this project, we will be using the VQA v2.0 dataset. Unlike VQA 1.0, which included both real and abstract scenes, VQA 2.0 only looks at real images. The task is also slightly different between versions - v1.0 included both open ended and multiple choice question answering, whereas v2.0 focuses exclusively on open ended question answering. \n",
    "\n",
    "<br> The VQA 2.0 dataset is a collection of 82,783 MS COCO training images, 40,504 MS COCO validation images and 81,434 MS COCO testing images. Each image has 3+ associated questions, for a total of 443,757 questions for training, 214,354 questions for validation and 447,793 questions for testing. Each question is associated with 10 ground truth answers, corresponding to the answers of 10 different human respondents when asked given the image-question pair. The dataset also includes a field identifying the most frequent ground truth answer of this set. <br>\n",
    "\n",
    "<br> Questions are broken into 3 sub-groups, based on their answer types: \"yes/no\", \"number\", and \"other.\" The VQA challenge reports model accuracy for each sub-group, as well as an overall number. \n",
    "\n",
    "Examples from the VQA v2.0 dataset <br>\n",
    "Question: What color is the hydrant? <br> <img src=\"FireHydrant.png\"> <br> Answer: Red\n",
    "\n",
    "\n",
    "\n",
    "Question:  What is hanging above the toilet? <br> <img src=\"TeddyBear.png\"> <br>  Answer: teddy bear\n",
    "\n",
    "<br> VQA 2.0 also includes a \"complementary pairs\" dataset. These are pairs of images that share the same question, but the answer to that question is different for each image (see below for example). Some [research](https://arxiv.org/pdf/1612.00837.pdf) has shown that training with this dataset improves model accuracy and prevents the model from overfitting to the most common answers. As of now, we are not using this dataset, but we may investigate using it as an extension.  <img src=\"PairedImages.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric\n",
    "\n",
    "The VQA Challenge has set up its own evaluation platform using EvalAI. The metric used for the challenge is <br> <br>\n",
    "$Acc(ans) = min \\{ \\frac{\\text{num humans that said ans} }{3}, 1 \\} $\n",
    "<br> The metric accounts for the fact that human respondents might give slightly different answers for a question. When asked \"what color is the scarf?\", one set of respondents might say \"blue\", while another set might say \"purple.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Steps -- Initial VQA Model\n",
    "\n",
    "As of now, we have loaded and run the original VQA model described in this [paper](https://arxiv.org/pdf/1505.00468.pdf). The model (which can be found [here](https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering)) is implemented in Keras with a Tensorflow backend. \n",
    "<br><br> \n",
    "We begin with all necessary import statements. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Flatten, Embedding, Merge, Input, Multiply\n",
    "from keras.layers.merge import Multiply\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Embedding\n",
    "Embeddings for the input image are taken from the last hidden layer of VGG19, which is a 4096 dimensional vector. Rather than run the images through VGG19 layers repeatedly, the authors of this network saved the embeddings for the images and use those as inputs to their network, rather than the raw images themselves. This is useful because this reduces computational intensity of the network, but it means that when we decide to change our CNN embedding, we will have to go back to using the raw images as input. \n",
    "<br><br>The 4096 element image embedding is then fed to a fully connected layer with 1024 output neurons and a tanh activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_model(dropout_rate):\n",
    "    print(\"Creating image model...\")\n",
    "    model = Sequential()\n",
    "    ##Feed the 4096 element image embedding through fully connected layer with 1024 output neurons and tanh activation\n",
    "    model.add(Dense(1024, input_dim=4096, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Using a previously trained GloVe embedding matrix, 300 element word2vec representations are created for each word in the question. The sequence of vectors is then fed to an LSTM with 2 hidden layers (with dropout applied). The output of this LSTM is then connected to a dense layer with 1024 output nerons and a tanh activation function. Because the embedding layer is instantiated with the trainable parameter set to false, the GloVe embedding matrix weights are not adjusted during training. \n",
    "<br> **Note:** The VQA paper says that it concatenates the last cell state and the last hidden state from both LSTM layers to form a 2048-dim embedding for the question, which is then fed to the 1024 unit FC layer. However, it is not clear from the code below that such a concatenation is being done. It looks like exclusively the last hidden state output from the 2nd hidden layer is being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate):\n",
    "    print(\"Creating text model...\")\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, embedding_dim, \n",
    "        weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
    "    model.add(LSTM(units=512, return_sequences=True, input_shape=(seq_length, embedding_dim)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units=512, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1024, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it together -- Combining Image and Word Embeddings\n",
    "Having created two 1024 dimensional embeddings, one for the image and one for the question, the model then merges these two. This merging is done via elementwise multiplication. \n",
    "The resulting 1024-element vector is then fed to a fully connected layer with 1000 output neurons and a tanh activation function. From there, it is fed to another fully connected layer with \"num_classes\" output neurons. \"num_classes\" represents the number of answers possible for the questions; each neuron maps to one answer. A softmax activation is used to reflect the fact that the final output is a probability vector whose elements sum to 1. The numerical value at any particular element in the output vector represents the probability that answer is the correct one for a particular image-question pair. The model's answer is the the answer with the max probability in the output vector. \n",
    "#### Loss function and training\n",
    "The model uses \"categorical_crossentropy\" as its loss function. This corresponds to the cross entropy metric defined in class; if $\\hat{y}$ is the softmax output reflecting probabilities weightings across all Z possible answers, then $y$ is a Z dimensions vector with a '1' at the position of the ground truth answer and '0' in all other positions. Given our specific dataset, the '1' is at the position of the most frequent answer given by the 10 human respondents.\n",
    "\n",
    "The model uses RMSprop as its optimization algorithm, with default hyperparameters (this is suggested in Keras documentation). Learning rate is 0.001, $\\rho$ is 0.9 (this was $\\beta$ in our lecture videos; the weighting of the current gradient relative to the historical average), $\\epsilon=1*10^{-8}$ (this is the 'fuzz' factor to protect against divide by zero errors), and learning rate decay is 0. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    vgg_model = img_model(dropout_rate)\n",
    "    lstm_model = Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate)\n",
    "    print(\"Merging final model...\")\n",
    "    fc_model = Sequential()\n",
    "    fc_model.add(Merge([vgg_model, lstm_model], mode='mul')) #Merge type layer now deprecated\n",
    "    fc_model.add(Dropout(dropout_rate))\n",
    "    fc_model.add(Dense(1000, activation='tanh'))\n",
    "    fc_model.add(Dropout(dropout_rate))\n",
    "    fc_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    #Setup loss function and defining training algorithm\n",
    "    fc_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return fc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Layer Deprecation\n",
    "In Keras 2.0, the Merge layer used in `vqa_model()` has been deprecated. The suggested fix would be to instead use a Multiply layer to perform the elementwise multiplication, but the Multiply layer will only accept tensors. Several workarounds were tried, including using the new Multiply layer with \"vgg_model.output\" and \"lstm_model.output\", but we ran into issues there because we were using \"symbolic tensors\" rather than actual tensors. \n",
    "\n",
    "The proper approach, recommanded by the Keras development community, is to switch from sequential style models, heavily utilized in Keras 1.0, to functional models, recommanded de-facto for Keras 2.0. Hence, we have redefined our models in functional Keras syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_model_func(dropout_rate):\n",
    "    print(\"Creating functional image model...\")\n",
    "    input_img = Input((4096,))\n",
    "    img_embedding = Dense(1024, input_dim=4096, activation='tanh')(input_img)\n",
    "    return input_img, img_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Word2VecModel_func(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate):\n",
    "    print(\"Creating functional text model...\")\n",
    "    input_q = Input((seq_length,))\n",
    "    x = Embedding(num_words, embedding_dim, weights=[embedding_matrix], trainable=False)(input_q)\n",
    "    x = LSTM(units=512, return_sequences=True)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = LSTM(units=512, return_sequences=False)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    q_embedding = Dense(1024, activation='tanh')(x)\n",
    "    return input_q, q_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vqa_model_func(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    input_img, img_embedding = img_model(dropout_rate)\n",
    "    input_q, q_embedding = Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate)\n",
    "    print(\"Merging final model...\")\n",
    "    combined = Multiply()([img_embedding, q_embedding])\n",
    "    combined = Dropout(dropout_rate)(combined)\n",
    "    combined = Dense(1000, activation='tanh')(combined)\n",
    "    combined = Dropout(dropout_rate)(combined)\n",
    "    predictions = Dense(num_classes, activation='softmax')(combined)\n",
    "    fc_model = Model(inputs=[input_img, input_q], outputs=predictions)\n",
    "    fc_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return fc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactoring to functional style models breaks the ability to load the pre-trained weights, trained on the sequential model definition.\n",
    "\n",
    "Therefore, to validate our functional model implementation, we trained the model from scratch overnight, and recorded evaluation results against the validation set at every 10 epochs.\n",
    "\n",
    "\n",
    "|  Epoch        | Accuracy            | True Postitve Rate   | Loss\n",
    "| :-------------: |:-------------| :-----|:-----|\n",
    "| 10     | 0.445552702614 | 0.573844558562 |2.53969596104|\n",
    "| 20      | 0.454802817829      |   0.58548949898 |2.59171895231|\n",
    "| 30 |         0.45152742116|0.582181183751  |2.6666429342|\n",
    "|40|0.458144051616|  0.587061360195 |2.76133057947\n",
    "|50 |0.453848179604 |0.581045493449| 2.9734828312\n",
    "|60 |0.449074988479 |0.577926459938 |3.08288818925\n",
    "|70 |0.450976035289 |0.577959378498 |3.19416170537\n",
    "|80 |0.449881493186 |0.577910000658 |3.32517784668\n",
    "|90 | 0.445272894858| 0.572231549147 |3.3803069139\n",
    "|100 |0.44325663309 |0.571375666601|3.44215128993\n",
    "\n",
    "Here, \"Accuracy\" measures the rate at which the predicted answers is the same as the top human answer.\n",
    "\n",
    "\"True Positive Rate\" measures the rate at which the predicted answers matches one of the top 10 human provided answers.\n",
    "\n",
    "Loss measures the categorical cross-entropy on the validation set.\n",
    "\n",
    "After training 30 epochs, our functional model achieved an accuracy on the validation set of 45.15%, which is within accpectable bounds to the accuracy of 45.03% reported by the original implementers of the sequential model. We believe the differences between our accuracies are caused by using difference batch_sizes. \n",
    "\n",
    "In addition, our model achieved highest accuracy after 40 epochs, with an accuracy of 45.81%. After 40 epochs, both the accuracy and true positive rate declines, indicating overfitting of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model \n",
    "Next we will run one training iteration of the model. Again, we begin with the necessary import statements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import argparse\n",
    "##from models import *  IGNORING THIS IMPORT, SINCE ALREADY DEFINED THE NECESSARY FUNCTIONS ABOVE\n",
    "from prepare_data import *\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the get model function which creates the model using our previously defined vqa_model function. Before that, it prepares the embedding matrix which will be used to generate the word2vec representations of each of the question words. This function also checks for previously saved weights and loads them if found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(dropout_rate, model_weights_filename):\n",
    "    print(\"Creating Model...\")\n",
    "    metadata = get_metadata()\n",
    "    num_classes = len(metadata['ix_to_ans'].keys())\n",
    "    num_words = len(metadata['ix_to_word'].keys())\n",
    "\n",
    "    embedding_matrix = prepare_embeddings(num_words, embedding_dim, metadata)\n",
    "    model = vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes)\n",
    "    if os.path.exists(model_weights_filename):\n",
    "        print(\"Loading Weights...\")\n",
    "        model.load_weights(model_weights_filename)\n",
    "    else:\n",
    "        print(\"No weights found at \" + model_weights_filename)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define functions for training and validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, batch_size, data_limit):\n",
    "    dropout_rate = 0.5\n",
    "    train_X, train_y = read_data(data_limit)    \n",
    "    model = get_model(dropout_rate, model_weights_filename)\n",
    "    checkpointer = ModelCheckpoint(filepath=ckpt_model_weights_filename, verbose=1, monitor='loss', save_best_only=True)\n",
    "    model.fit(train_X, train_y, epochs=epoch, batch_size=batch_size, callbacks=[checkpointer], shuffle=\"batch\")\n",
    "    print(\"Training Complete!\")\n",
    "    #For now, disable final save weights since we've already saved the best model to date\n",
    "    #model.save_weights(ckpt_model_weights_filename + \".final\", overwrite=True)\n",
    "    \n",
    "def val():\n",
    "    val_X, val_y, multi_val_y = get_val_data() \n",
    "    model = get_model(0.0, model_weights_filename)\n",
    "    print(\"Evaluating Accuracy on validation set:\")\n",
    "    metric_vals = model.evaluate(val_X, val_y)\n",
    "    print(\"\")\n",
    "    for metric_name, metric_val in zip(model.metrics_names, metric_vals):\n",
    "        print(str(metric_name) + \" is \" + str(metric_val))\n",
    "\n",
    "    # Comparing prediction against multiple choice answers\n",
    "    true_positive = 0\n",
    "    preds = model.predict(val_X)\n",
    "    pred_classes = [np.argmax(_) for _ in preds]\n",
    "    for i, _ in enumerate(pred_classes):\n",
    "        if _ in multi_val_y[i]:\n",
    "            true_positive += 1\n",
    "    print(\"True positive rate: \" +  str(np.float(true_positive)/len(pred_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a test of the validation function to see how well our model does with the weights provided by the model authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test the training function. For a quick test, lets only run 1 epoch with a batch size of 10, and only use 10 question-image pairs (instead of the full training set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epoch=3, batch_size=10, data_limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
