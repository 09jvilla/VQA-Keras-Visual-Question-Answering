{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS230 Project\n",
    "# Deep Learning for VQA: Visual Question Answering\n",
    "\n",
    "Stephanie Do <br> Alona King <br> Jennifer Villa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project explores the challenge of visual question answering (VQA) -- given an image and an open ended question concerning the image, build a model that returns a correct answer. This topic requires synthesizing both visual and language modalities, and combining the two to produce a natural language answer, making it more challenging than traditional image classification. VQA challenges researchers to create networks with a more sophisticated level of understanding that could ultimately be used to help robots or drones navigate their environment. These networks could also give visually impaired people a more rich description of a scene, or be used for better image or product search within a database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "For this project, we will be using the VQA v2.0 dataset. Unlike VQA 1.0, which included both real and abstract scenes, VQA 2.0 only looks at real images. The task is also slightly different between versions - v1.0 included both open ended and multiple choice question answering, whereas v2.0 focuses exclusively on open ended question answering. \n",
    "\n",
    "<br> The VQA 2.0 dataset is a collection of 82,783 MS COCO training images, 40,504 MS COCO validation images and 81,434 MS COCO testing images. Each image has 3+ associated questions, for a total of 443,757 questions for training, 214,354 questions for validation and 447,793 questions for testing. Each question is associated with 10 ground truth answers, corresponding to the answers of 10 different human respondents when asked given the image-question pair. The dataset also includes a field identifying the most frequent ground truth answer of this set. <br>\n",
    "\n",
    "<br> Questions are broken into 3 sub-groups, based on their answer types: \"yes/no\", \"number\", and \"other.\" The VQA challenge reports model accuracy for each sub-group, as well as an overall number. \n",
    "\n",
    "Examples from the VQA v2.0 dataset <br>\n",
    "Question: What color is the hydrant? <br> <img src=\"FireHydrant.png\"> <br> Answer: Red\n",
    "\n",
    "\n",
    "\n",
    "Question:  What is hanging above the toilet? <br> <img src=\"TeddyBear.png\"> <br>  Answer: teddy bear\n",
    "\n",
    "<br> VQA 2.0 also includes a \"complementary pairs\" dataset. These are pairs of images that share the same question, but the answer to that question is different for each image (see below for example). Some [research](https://arxiv.org/pdf/1612.00837.pdf) has shown that training with this dataset improves model accuracy and prevents the model from overfitting to the most common answers. As of now, we are not using this dataset, but we may investigate using it as an extension.  <img src=\"PairedImages.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric\n",
    "\n",
    "The VQA Challenge has set up its own evaluation platform using EvalAI. The metric used for the challenge is <br> <br>\n",
    "$Acc(ans) = min \\{ \\frac{\\text{num humans that said ans} }{3}, 1 \\} $\n",
    "<br> The metric accounts for the fact that human respondents might give slightly different answers for a question. When asked \"what color is the scarf?\", one set of respondents might say \"blue\", while another set might say \"purple.\" If at least 3 of the 10 respondents give a particular answer, the answer is considered a full credit answer. Otherwise, fractional credit is awarded based on the number of people who gave that answer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Steps -- Initial VQA Model\n",
    "\n",
    "As of now, we have loaded and run the original VQA model described in this [paper](https://arxiv.org/pdf/1505.00468.pdf). The model (which can be found [here](https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering)) is implemented in Keras with a Tensorflow backend. \n",
    "<br><br> \n",
    "We begin with all necessary import statements. Please note that you need Keras, Tensorflow, and h5py installed to run this code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/jennifervilla/Documents/tensorflow_python3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Flatten, Embedding, Merge, Input, Multiply, Concatenate, Lambda\n",
    "from keras.layers.merge import Multiply\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Dataset\n",
    "We begin with the dataset described above. You will need to download the following: <br>\n",
    "1. [GloVe Embeddings](http://nlp.stanford.edu/data/glove.6B.zip)\n",
    "2. [Training Data](https://filebox.ece.vt.edu/~jiasenlu/codeRelease/vqaRelease/train_only/data_train_val.zip)\n",
    "3. [Validation Data](http://visualqa.org/data/mscoco/vqa/Annotations_Val_mscoco.zip)\n",
    "4. [Pre-trained Model Weights](https://drive.google.com/file/d/0B3b69xdtpDT8U2dwajNKOEhUWUU/view?usp=sharing)\n",
    "\n",
    "Having downloaded these files, create a `data` directory within the same directory you are running this notebook. Extract these zip files into this data directory so that you have the following files/directories in `data`:\n",
    "1. data_img.h5\n",
    "2. data_prepro.h5\n",
    "3. dada_prepro.json\n",
    "4. glove.6B.300d.txt\n",
    "5. Questions_Train_mscoco/MultipleChoice_mscoco_train2014_questions.json\n",
    "6. validation_data/mscoco_val2014_annotations.json\n",
    "7. model_weights.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having set up the data, we now set the following constants relating to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length =    26\n",
    "embedding_dim = 300\n",
    "\n",
    "data_img =                  'data/data_img.h5'\n",
    "data_prepo =                'data/data_prepro.h5'\n",
    "data_prepo_meta =           'data/data_prepro.json'\n",
    "embedding_matrix_filename = 'data/ckpts/embeddings_%s.h5'%embedding_dim\n",
    "glove_path =                'data/glove.6B.300d.txt'\n",
    "train_questions_path =      'data/Questions_Train_mscoco/MultipleChoice_mscoco_train2014_questions.json'\n",
    "val_annotations_path =      'data/validation_data/mscoco_val2014_annotations.json'\n",
    "ckpt_model_weights_filename =    'data/ckpts/trained_model_weights.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the functions for preprocessing the data. These were functions taken from the published model, with a few tweaks (fixed 1-2 small bugs and broken paths; fixes for python 3 instead of python 2 usage). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import json\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "def right_align(seq,lengths):\n",
    "    v = np.zeros(np.shape(seq))\n",
    "    N = np.shape(seq)[1]\n",
    "    for i in range(np.shape(seq)[0]):\n",
    "        v[i][N-lengths[i]:N]=seq[i][0:lengths[i]]\n",
    "    return v\n",
    "\n",
    "def read_data(data_limit):\n",
    "    print(\"Reading Data...\")\n",
    "    img_data = h5py.File(data_img)\n",
    "    ques_data = h5py.File(data_prepo)\n",
    "  \n",
    "    img_data = np.array(img_data['images_train'])\n",
    "    img_pos_train = ques_data['img_pos_train'][:data_limit]\n",
    "    train_img_data = np.array([img_data[_-1,:] for _ in img_pos_train])\n",
    "    # Normalizing images\n",
    "    tem = np.sqrt(np.sum(np.multiply(train_img_data, train_img_data), axis=1))\n",
    "    train_img_data = np.divide(train_img_data, np.transpose(np.tile(tem,(4096,1))))\n",
    "\n",
    "    #shifting padding to left side\n",
    "    ques_train = np.array(ques_data['ques_train'])[:data_limit, :]\n",
    "    ques_length_train = np.array(ques_data['ques_length_train'])[:data_limit]\n",
    "    ques_train = right_align(ques_train, ques_length_train)\n",
    "\n",
    "    train_X = [train_img_data, ques_train]\n",
    "    # NOTE should've consturcted one-hots using exhausitve list of answers, cause some answers may not be in dataset\n",
    "    # To temporarily rectify this, all those answer indices is set to 1 in validation set\n",
    "    train_y = to_categorical(ques_data['answers'])[:data_limit, :]\n",
    "\n",
    "    return train_X, train_y\n",
    "\n",
    "def get_val_data():\n",
    "    img_data = h5py.File(data_img)\n",
    "    ques_data = h5py.File(data_prepo)\n",
    "    metadata = get_metadata()\n",
    "    with open(val_annotations_path, 'r') as an_file:\n",
    "        annotations = json.loads(an_file.read())\n",
    "\n",
    "    img_data = np.array(img_data['images_test'])\n",
    "    img_pos_train = ques_data['img_pos_test']\n",
    "    train_img_data = np.array([img_data[_-1,:] for _ in img_pos_train])\n",
    "    tem = np.sqrt(np.sum(np.multiply(train_img_data, train_img_data), axis=1))\n",
    "    train_img_data = np.divide(train_img_data, np.transpose(np.tile(tem,(4096,1))))\n",
    "\n",
    "    ques_train = np.array(ques_data['ques_test'])\n",
    "    ques_length_train = np.array(ques_data['ques_length_test'])\n",
    "    ques_train = right_align(ques_train, ques_length_train)\n",
    "\n",
    "    # Convert all last index to 0, coz embeddings were made that way :/\n",
    "    for _ in ques_train:\n",
    "        if 12602 in _:\n",
    "            _[_==12602] = 0\n",
    "\n",
    "    val_X = [train_img_data, ques_train]\n",
    "\n",
    "    ans_to_ix = {str(ans):int(i) for i,ans in metadata['ix_to_ans'].items()}\n",
    "    ques_annotations = {}\n",
    "    for _ in annotations['annotations']:\n",
    "        idx = ans_to_ix.get(_['multiple_choice_answer'].lower())\n",
    "        _['multiple_choice_answer_idx'] = 1 if idx in [None, 1000] else idx\n",
    "        ques_annotations[_['question_id']] = _\n",
    "\n",
    "    abs_val_y = [ques_annotations[ques_id]['multiple_choice_answer_idx'] for ques_id in ques_data['question_id_test']]\n",
    "    abs_val_y = to_categorical(np.array(abs_val_y))\n",
    "\n",
    "    multi_val_y = [list(set([ans_to_ix.get(_['answer'].lower()) for _ in ques_annotations[ques_id]['answers']])) for ques_id in ques_data['question_id_test']]\n",
    "    for i,_ in enumerate(multi_val_y):\n",
    "        multi_val_y[i] = [1 if ans in [None, 1000] else ans for ans in _]\n",
    "\n",
    "    return val_X, abs_val_y, multi_val_y\n",
    "\n",
    "\n",
    "def get_metadata():\n",
    "    meta_data = json.load(open(data_prepo_meta, 'r'))\n",
    "    meta_data['ix_to_word'] = {str(word):int(i) for i,word in meta_data['ix_to_word'].items()}\n",
    "    return meta_data\n",
    "\n",
    "def prepare_embeddings(num_words, embedding_dim, metadata):\n",
    "    if os.path.exists(embedding_matrix_filename):\n",
    "        with h5py.File(embedding_matrix_filename) as f:\n",
    "            return np.array(f['embedding_matrix'])\n",
    "\n",
    "    print(\"Embedding Data...\")\n",
    "    with open(train_questions_path, 'r') as qs_file:\n",
    "        questions = json.loads(qs_file.read())\n",
    "        texts = [str(_['question']) for _ in questions['questions']]\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, 'r') as glove_file:\n",
    "        for line in glove_file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    word_index = metadata['ix_to_word']\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "   \n",
    "    with h5py.File(embedding_matrix_filename, 'w') as f:\n",
    "        f.create_dataset('embedding_matrix', data=embedding_matrix)\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Embedding\n",
    "Embeddings for the input image are taken from the last hidden layer of VGG19, which is a 4096 dimensional vector. Rather than run the images through VGG19 layers repeatedly, the authors of this network saved the embeddings for the images and use those as inputs to their network, rather than the raw images themselves. This is useful because this reduces computational intensity of the network, but it means that when we decide to change our CNN embedding, we will have to go back to using the raw images as input. \n",
    "<br><br>The 4096 element image embedding is then fed to a fully connected layer with 1024 output neurons and a tanh activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_model(dropout_rate):\n",
    "    print(\"Creating image model...\")\n",
    "    model = Sequential()\n",
    "    ##Feed the 4096 element image embedding through fully connected layer with 1024 output neurons and tanh activation\n",
    "    model.add(Dense(1024, input_dim=4096, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='word_embedding_note'></a>\n",
    "### Word Embedding\n",
    "Using a previously trained GloVe embedding matrix, 300 element word2vec representations are created for each word in the question. The sequence of vectors is then fed to an LSTM with 2 hidden layers (with dropout applied). The output of this LSTM is then connected to a dense layer with 1024 output nerons and a tanh activation function. Because the embedding layer is instantiated with the trainable parameter set to false, the GloVe embedding matrix weights are not adjusted during training. \n",
    "<br> **Note:** The VQA paper says that it concatenates the last cell state and the last hidden state from both LSTM layers to form a 2048-dim embedding for the question, which is then fed to the 1024 unit FC layer. However, it is not clear from the code below that such a concatenation is being done. It looks like exclusively the last hidden state output from the 2nd hidden layer is being used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate):\n",
    "    print(\"Creating text model...\")\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, embedding_dim, \n",
    "        weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
    "    model.add(LSTM(units=512, return_sequences=True, input_shape=(seq_length, embedding_dim)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units=512, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1024, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it together -- Combining Image and Word Embeddings\n",
    "Having created two 1024 dimensional embeddings, one for the image and one for the question, the model then merges these two. This merging is done via elementwise multiplication. \n",
    "The resulting 1024-element vector is then fed to a fully connected layer with 1000 output neurons and a tanh activation function. From there, it is fed to another fully connected layer with \"num_classes\" output neurons. \"num_classes\" represents the number of answers possible for the questions; each neuron maps to one answer. A softmax activation is used to reflect the fact that the final output is a probability vector whose elements sum to 1. The numerical value at any particular element in the output vector represents the probability that answer is the correct one for a particular image-question pair. The model's answer is the the answer with the max probability in the output vector. \n",
    "#### Loss function and training\n",
    "The model uses \"categorical_crossentropy\" as its loss function. This corresponds to the cross entropy metric defined in class; if $\\hat{y}$ is the softmax output reflecting probabilities weightings across all Z possible answers, then $y$ is a Z dimensions vector with a '1' at the position of the ground truth answer and '0' in all other positions. Given our specific dataset, the '1' is at the position of the most frequent answer given by the 10 human respondents.\n",
    "\n",
    "The model uses RMSprop as its optimization algorithm, with default hyperparameters (this is suggested in Keras documentation). Learning rate is 0.001, $\\rho$ is 0.9 (this was $\\beta$ in our lecture videos; the weighting of the current gradient relative to the historical average), $\\epsilon=1*10^{-8}$ (this is the 'fuzz' factor to protect against divide by zero errors), and learning rate decay is 0. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    vgg_model = img_model(dropout_rate)\n",
    "    lstm_model = Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate)\n",
    "    print(\"Merging final model...\")\n",
    "    fc_model = Sequential()\n",
    "    fc_model.add(Merge([vgg_model, lstm_model], mode='mul')) #Merge type layer now deprecated\n",
    "    fc_model.add(Dropout(dropout_rate))\n",
    "    fc_model.add(Dense(1000, activation='tanh'))\n",
    "    fc_model.add(Dropout(dropout_rate))\n",
    "    fc_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    #Setup loss function and defining training algorithm\n",
    "    fc_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return fc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "To test our model, we ran one training iteration of the model. Again, we begin with the necessary import statements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the get model function which creates the model using our previously defined vqa_model function. Before that, it prepares the embedding matrix which will be used to generate the word2vec representations of each of the question words. This function also checks for previously saved weights and loads them if found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dropout_rate, model_weights_filename=\"\"):\n",
    "    print(\"Creating Model...\")\n",
    "    metadata = get_metadata()\n",
    "    num_classes = len(metadata['ix_to_ans'].keys())\n",
    "    num_words = len(metadata['ix_to_word'].keys())\n",
    "\n",
    "    embedding_matrix = prepare_embeddings(num_words, embedding_dim, metadata)\n",
    "    model = vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes)\n",
    "    if model_weights_filename==\"\":\n",
    "        print(\"No model weights file specified. Skipping weight load.\")\n",
    "    elif os.path.exists(model_weights_filename):\n",
    "        print(\"Loading Weights...\")\n",
    "        model.load_weights(model_weights_filename)\n",
    "    else:\n",
    "        print(\"Could not find file: \" + model_weights_filename + \" for loading. Skipping weight load.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define functions for training and validation. Each has a `model_weights_filename` parameter that can be specified if you want the model to preload a specific set of weights. Currently the knobs are set so that our model saves after each epoch of training, overwriting previous iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, batch_size, data_limit, model_weights_filename=\"\"):\n",
    "    dropout_rate = 0.5\n",
    "    train_X, train_y = read_data(data_limit)        \n",
    "    model = get_model(dropout_rate, model_weights_filename)\n",
    "    checkpointer = ModelCheckpoint(filepath=ckpt_model_weights_filename, verbose=1, monitor='loss', save_best_only=False)\n",
    "    model.fit(train_X, train_y, epochs=epoch, batch_size=batch_size, callbacks=[checkpointer], shuffle=\"batch\")\n",
    "    return model\n",
    "    \n",
    "def val(model_weights_filename=\"\"):\n",
    "    if model_weights_filename==\"\":\n",
    "        print(\"No weights specified. Validating model with randomly initialized weights.\")\n",
    "        \n",
    "    val_X, val_y, multi_val_y = get_val_data() \n",
    "    model = get_model(0.0, model_weights_filename)\n",
    "    print(\"Evaluating Accuracy on validation set:\")\n",
    "    metric_vals = model.evaluate(val_X, val_y)\n",
    "    print(\"\")\n",
    "    for metric_name, metric_val in zip(model.metrics_names, metric_vals):\n",
    "        print(str(metric_name) + \" is \" + str(metric_val))\n",
    "\n",
    "    # Comparing prediction against multiple choice answers\n",
    "    true_positive = 0\n",
    "    preds = model.predict(val_X)\n",
    "    pred_classes = [np.argmax(_) for _ in preds]\n",
    "    for i, _ in enumerate(pred_classes):\n",
    "        if _ in multi_val_y[i]:\n",
    "            true_positive += 1\n",
    "    print(\"True positive rate: \" +  str(np.float(true_positive)/len(pred_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a test of the validation function to see how well our model does with the weights provided by the model authors. After running the line of code below to test the model on the entire validation set, the following results were obtained:\n",
    "<br><br>\n",
    "**Loss:** 2.76330976921 <br>\n",
    "**Accuracy:** 0.460777536375\n",
    "<br><br>\n",
    "This is approximately equivalent to the 45.03% validation accuracy reported in the original author's documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val(model_weights_filename=\"data/model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test the training function. For a quick test, lets only run 3 epochs with a batch size of 10, and only use 10 question-image pairs (instead of the full training set). You can switch the value of the load_weights parameter if you would like to train from the frozen weights or from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data...\n",
      "Creating Model...\n",
      "Creating image model...\n",
      "Creating text model...\n",
      "Merging final model...\n",
      "No model weights file specified. Skipping weight load.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennifervilla/Documents/tensorflow_python3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Epoch 00001: saving model to data/ckpts/trained_model_weights.h5\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 6.9079 - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "Epoch 00002: saving model to data/ckpts/trained_model_weights.h5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 6.8000 - acc: 0.7000\n",
      "Epoch 3/3\n",
      "Epoch 00003: saving model to data/ckpts/trained_model_weights.h5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 5.8906 - acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x120cf9710>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(epoch=3, batch_size=10, data_limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model...\n",
      "Creating image model...\n",
      "Creating text model...\n",
      "Merging final model...\n",
      "Loading Weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennifervilla/Documents/tensorflow_python3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy on validation set:\n",
      "   480/121512 [..............................] - ETA: 14:53"
     ]
    }
   ],
   "source": [
    "val(model_weights_filename=\"data/ckpts/trained_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrating to a Functional-Style Keras Model\n",
    "### Merge Layer Deprecation\n",
    "When we ran the initial model, we saw warnings that as of Keras 2.0, the Merge layer used in `vqa_model()` has been deprecated. The suggested fix would be to instead use a Multiply layer to perform the elementwise multiplication, but the Multiply layer will only accept tensors. Several workarounds were tried, including using the new Multiply layer with \"vgg_model.output\" and \"lstm_model.output\", but we ran into issues there because we were using \"symbolic tensors\" rather than actual tensors. \n",
    "\n",
    "The proper approach, recommended by the Keras development community, is to switch from sequential style models, heavily utilized in Keras 1.0, to functional models, recommanded de-facto for Keras 2.0. Hence, we have redefined our models in functional Keras syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_model(dropout_rate):\n",
    "    print(\"Creating functional image model...\")\n",
    "    input_img = Input((4096,))\n",
    "    img_embedding = Dense(1024, input_dim=4096, activation='tanh')(input_img)\n",
    "    return input_img, img_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate):\n",
    "    print(\"Creating functional text model...\")\n",
    "    input_q = Input((seq_length,))\n",
    "    x = Embedding(num_words, embedding_dim, weights=[embedding_matrix], trainable=False)(input_q)\n",
    "    x = LSTM(units=512, return_sequences=True)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = LSTM(units=512, return_sequences=False)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    q_embedding = Dense(1024, activation='tanh')(x)\n",
    "    return input_q, q_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    input_img, img_embedding = img_model(dropout_rate)\n",
    "    input_q, q_embedding = Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate)\n",
    "    print(\"Merging final functional model...\")\n",
    "    combined = Multiply()([img_embedding, q_embedding])\n",
    "    combined = Dropout(dropout_rate)(combined)\n",
    "    combined = Dense(1000, activation='tanh')(combined)\n",
    "    combined = Dropout(dropout_rate)(combined)\n",
    "    predictions = Dense(num_classes, activation='softmax')(combined)\n",
    "    fc_model = Model(inputs=[input_img, input_q], outputs=predictions)\n",
    "    fc_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return fc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactoring to functional style models breaks the ability to load the pre-trained weights, which were trained on the sequential model definition.\n",
    "\n",
    "Therefore, to validate our functional model implementation, we trained the model from scratch overnight, and recorded evaluation results against the validation set at every 10 epochs. Keras has support to evaluate the validation set, but only for every single epoch, so we wrote our own function instead.  We also printed out a heartbeat as the training continued.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(num_loops, epoch, batch_size, data_limit):\n",
    "    wfilename = ckpt_model_weights_filename\n",
    "    \n",
    "    for i in range(1, num_loops + 1):\n",
    "        #First time through, do not preload weights\n",
    "        if i == 1:\n",
    "            model = train(epoch, batch_size, data_limit, \"\")\n",
    "        \n",
    "        #Second time through, preload weights from prior training\n",
    "        else:    \n",
    "            model = train(epoch, batch_size, data_limit, wfilename)\n",
    "        \n",
    "        #Evaluate model on the most recent weight values\n",
    "        metrics, true_positive_rate = val(wfilename)\n",
    "        with open(\"training_log\", \"a\") as val_log:\n",
    "            val_log.write(\"After training epoch \" + str(epoch * i)+\"\\n\")\n",
    "            for name, value in metrics:\n",
    "                val_log.write(name + \" \" + str(value)+\"\\n\")\n",
    "            val_log.write(\"True_positive_rate: \" + str(true_positive_rate)+\"\\n\")\n",
    "        print(\"Finished loop number: \", i)\n",
    "    \n",
    "    print(\"Full training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the loop function to test it. Again, we keep all parameters small in order to run quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data...\n",
      "Creating Model...\n",
      "Creating functional image model...\n",
      "Creating functional text model...\n",
      "Merging final model...\n",
      "No model weights file specified. Skipping weight load.\n",
      "Epoch 1/2\n",
      "Epoch 00001: saving model to data/ckpts/trained_model_weights.h5\n",
      "10/10 [==============================] - 3s 332ms/step - loss: 6.9080 - acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "Epoch 00002: saving model to data/ckpts/trained_model_weights.h5\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 6.8338 - acc: 0.5000\n",
      "Creating Model...\n",
      "Creating functional image model...\n",
      "Creating functional text model...\n",
      "Merging final model...\n",
      "Loading Weights...\n",
      "Evaluating Accuracy on validation set:\n",
      "   544/121512 [..............................] - ETA: 24:12"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e75b67faf3cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_loops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-e9dfa99a2df5>\u001b[0m in \u001b[0;36mloop\u001b[0;34m(num_loops, epoch, batch_size, data_limit)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#Evaluate model on the most recent weight values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_positive_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mval_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mval_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"After training epoch \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7f087bcffb93>\u001b[0m in \u001b[0;36mval\u001b[0;34m(model_weights_filename)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_weights_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating Accuracy on validation set:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmetric_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tensorflow_python3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1690\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m                                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m                                steps=steps)\n\u001b[0m\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m~/Documents/tensorflow_python3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tensorflow_python3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tensorflow_python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tensorflow_python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tensorflow_python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tensorflow_python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tensorflow_python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loop(num_loops=2, epoch=2, batch_size=10, data_limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon running the `loop` function on the full dataset for 100 epochs, our results were as follows: <br>\n",
    "\n",
    "|  Epoch        | Accuracy            | True Postitve Rate   | Loss\n",
    "| :-------------: |:-------------| :-----|:-----|\n",
    "| 10     | 0.445552702614 | 0.573844558562 |2.53969596104|\n",
    "| 20      | 0.454802817829      |   0.58548949898 |2.59171895231|\n",
    "| 30 |         0.45152742116|0.582181183751  |2.6666429342|\n",
    "|40|0.458144051616|  0.587061360195 |2.76133057947\n",
    "|50 |0.453848179604 |0.581045493449| 2.9734828312\n",
    "|60 |0.449074988479 |0.577926459938 |3.08288818925\n",
    "|70 |0.450976035289 |0.577959378498 |3.19416170537\n",
    "|80 |0.449881493186 |0.577910000658 |3.32517784668\n",
    "|90 | 0.445272894858| 0.572231549147 |3.3803069139\n",
    "|100 |0.44325663309 |0.571375666601|3.44215128993\n",
    "\n",
    "Here, **Accuracy** measures the rate at which the predicted answer is the same as the top human answer.\n",
    "\n",
    "**True Positive Rate** measures the rate at which the predicted answer matches any one of the 10 human provided answers.\n",
    "\n",
    "**Loss** measures the categorical cross-entropy on the validation set.\n",
    "\n",
    "After training 30 epochs, our functional model achieved an accuracy on the validation set of 45.15%, which is almost identical to the 45.03% accuracy figure reported by the original implementers of the sequential model. We believe the differences between our accuracies are caused by using difference batch_sizes. \n",
    "\n",
    "Our model achieved highest accuracy after 40 epochs, with an accuracy of 45.81%. After 40 epochs, both the accuracy and true positive rate declines, indicating overfitting of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Next Steps\n",
    "1. **Test different image embeddings**: Currently, our model uses image embeddings taken from the last hidden layer of VGG19. We would like to test the performance of embeddings taken from other well-known CNN models. Keras has [open source implementations](https://github.com/fchollet/keras/tree/master/keras/applications) of Resnet-50, Inception-Resnet_v2, Inception_v3, and Xception. A search of Github also reveals a Keras [Resnet-152](https://gist.github.com/flyyufelix/7e2eafb149f72f4d38dd661882c554a6) implementation, which might also be worth trying since this was the [model](https://web.stanford.edu/class/cs224n/reports/2748290.pdf) used by another Stanford team. As with our existing model, it probably makes sense to pre-compute the image features by running the VQA dataset images through the chosen network for embedding. Using our new set of image features, we can re-train our model to see if we can achieve better accuracy. \n",
    "2. **Implement New Cost Function for Soft Cross Entropy Loss**: Currently our model uses binary cross-entropy loss, with the ground-truth answer being a one-hot vector encoding the answer given by the majority of the human respondents polled. However, the VQA metric actually awards partial credit to models that output answers that match any of the ten human respondent answers. Thus, there is currently a disconnect between the loss function and the evaluation metric. One of the top performers in this year's VQA challenge sought to address this by proposing a [soft cross entropy loss](https://ilija139.github.io/pub/cvpr2017_vqa.pdf) function. This function calculates a weighted average of all unique ground truth answers given by the 10 human respondents. In the paper, these researchers achieved ~1.2-1.6% improved accuracy across a variety of model architectures. We would like to see if this improvement translates to our model as well. \n",
    "3. **Implement Multi-Modal Factorized Bilinear Pooling**: The [second place winner](https://arxiv.org/pdf/1708.01471.pdf) in this year’s VQA challenge proposed the concept of multi-modal factorized bilinear pooling. The idea is that using concatenation or elementwise multiplication to combine image and question embeddings (each of which represents information from a different modality, hence the term “multi-modal”) may limit model performance. A more sophisticated approach to fusing these two might be necessary. Thus they proposed “multi-modal factorized bilinear pooling”, which amounts to a combination of element-wise multiplication, fully connected layers, and sum pooling. We would like to see the impact of substituting this technique in place of our current “Multiply” layer in our VQA model. Once we implement co-attention (see $4$), we could also incorporate this technique into that model. (Authors of the original paper saw improvement in both simpler model architectures, such as the one we currently have, and those with co-attention).\n",
    "4. **Implement Image-Question Co-attention**: All of the top performing VQA models use some form of attention. While early research focused exclusively on image attention [[1]](https://arxiv.org/abs/1511.07394), more recent work has combined image attention with question attention [[2]](https://arxiv.org/abs/1606.00061) [[3]](https://web.stanford.edu/class/cs224n/reports/2748290.pdf). The philosophy behind the latter approach is just as certain regions of the image are more relevant than others, certain words in the question are more helpful in answering than others. A variety of different co-attention architectures have been proposed, including hierarchical co-attention, which looks at question attention recursively on word, phrase, and full question levels. Other co-attention models (like [3]) include information indicating the part of speech of a word. We plan on starting with an architecture like the one proposed in the \"Multi-Modal Factorized Bilinear Pooling\" paper, which won second place [[4]](https://arxiv.org/pdf/1708.01471.pdf). The experimenters provide their [source code](https://github.com/yuzcccc/vqa-mfb), but it is written in Caffe. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed in the [**Note**](#word_embedding_note), the word embeddings taken from the open source code we imported only used the last hidden state output of the LSTM (i.e. output from the second layer). Now we'll fix that and retrain to see if our accuracy is improved by using the cell ('c') and hidden ('h') output of both LSTM hidden layers. This is what the original [VQA paper](https://arxiv.org/pdf/1505.00468v6.pdf) describes doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate):\n",
    "    print(\"Creating functional text model with both layers...\")\n",
    "    input_q = Input((seq_length,))\n",
    "    x = Embedding(num_words, embedding_dim, weights=[embedding_matrix], trainable=False)(input_q)\n",
    "    lstm1, state_h1, state_c1 = LSTM(units=512, return_sequences=True, return_state=True)(x)\n",
    "    #lstm1_d = Dropout(dropout_rate)(lstm1)\n",
    "    \n",
    "    lstm2, state_h2, state_c2 = LSTM(units=512, return_sequences=False, return_state=True)(lstm1)    \n",
    "    #lstm2_d = Dropout(dropout_rate)(lstm2)\n",
    "    \n",
    "    #print(\"state_h1 state_c1 \" + str(state_h1.get_shape()) + \" \" + str(state_c1.get_shape()))\n",
    "    #print(\"state_h2 state_c2 \" + str(state_h2.get_shape()) + \" \" + str(state_c2.get_shape()))\n",
    "    \n",
    "    concat = Concatenate()([state_h1, state_c1, state_h2, state_c2])\n",
    "    #print(\"concat shape: \" + str(concat.get_shape()) )\n",
    "    \n",
    "    q_embedding = Dense(1024, activation='tanh')(concat)\n",
    "    #print(\"q_embedding: \" + str(q_embedding.get_shape()) )\n",
    "    \n",
    "    return input_q, q_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data...\n",
      "Creating Model...\n",
      "Creating functional image model...\n",
      "Creating functional text model with both layers...\n",
      "Merging final functional model...\n",
      "No model weights file specified. Skipping weight load.\n",
      "Epoch 1/2\n",
      "10/20 [==============>...............] - ETA: 3s - loss: 6.9083 - acc: 0.0000e+00Epoch 00001: saving model to data/ckpts/trained_model_weights.h5\n",
      "20/20 [==============================] - 5s 234ms/step - loss: 6.8268 - acc: 0.1500\n",
      "Epoch 2/2\n",
      "10/20 [==============>...............] - ETA: 0s - loss: 5.6916 - acc: 0.1000Epoch 00002: saving model to data/ckpts/trained_model_weights.h5\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 4.2208 - acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x146a28128>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(2, 10, 20, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
