{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS230 Project\n",
    "# Deep Learning for VQA: Visual Question Answering\n",
    "\n",
    "Stephanie Do <br> Alona King <br> Jennifer Villa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project explores the challenge of visual question answering (VQA) -- given an image and an open ended question concerning the image, build a model that returns a correct answer. This topic requires synthesizing both visual and language modalities, and combining the two to produce a natural language answer, making it more challenging than traditional image classification. VQA challenges researchers to create networks with a more sophisticated level of understanding that could ultimately be used to help robots or drones navigate their environment. These networks could also give visually impaired people a more rich description of a scene, or be used for better image or product search within a database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "For this project, we will be using the VQA v2.0 dataset. Unlike VQA 1.0, which included both real and abstract scenes, VQA 2.0 only looks at real images. The task is also slightly different between versions - v1.0 included both open ended and multiple choice question answering, whereas v2.0 focuses exclusively on open ended question answering. \n",
    "\n",
    "<br> The VQA 2.0 dataset is a collection of 82,783 MS COCO training images, 40,504 MS COCO validation images and 81,434 MS COCO testing images. Each image has 3+ associated questions, for a total of 443,757 questions for training, 214,354 questions for validation and 447,793 questions for testing. Each question is associated with 10 ground truth answers, corresponding to the answers of 10 different human respondents when asked given the image-question pair. The dataset also includes a field identifying the most frequent ground truth answer of this set. <br>\n",
    "\n",
    "<br> Questions are broken into 3 sub-groups, based on their answer types: \"yes/no\", \"number\", and \"other.\" The VQA challenge reports model accuracy for each sub-group, as well as an overall number. \n",
    "\n",
    "Examples from the VQA v2.0 dataset <br>\n",
    "Question: What color is the hydrant? <br> <img src=\"FireHydrant.png\"> <br> Answer: Red\n",
    "\n",
    "\n",
    "\n",
    "Question:  What is hanging above the toilet? <br> <img src=\"TeddyBear.png\"> <br>  Answer: teddy bear\n",
    "\n",
    "<br> VQA 2.0 also includes a \"complementary pairs\" dataset. These are pairs of images that share the same question, but the answer to that question is different for each image (see below for example). Some [research](https://arxiv.org/pdf/1612.00837.pdf) has shown that training with this dataset improves model accuracy and prevents the model from overfitting to the most common answers. As of now, we are not using this dataset, but we may investigate using it as an extension.  <img src=\"PairedImages.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric\n",
    "\n",
    "The VQA Challenge has set up its own evaluation platform using EvalAI. The metric used for the challenge is <br> <br>\n",
    "$Acc(ans) = min \\{ \\frac{\\text{num humans that said ans} }{3}, 1 \\} $\n",
    "<br> The metric accounts for the fact that human respondents might give slightly different answers for a question. When asked \"what color is the scarf?\", one set of respondents might say \"blue\", while another set might say \"purple.\" If at least 3 of the 10 respondents give a particular answer, the answer is considered a full credit answer. Otherwise, fractional credit is awarded based on the number of people who gave that answer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Steps -- Initial VQA Model\n",
    "\n",
    "As of now, we have loaded and run the original VQA model described in this [paper](https://arxiv.org/pdf/1505.00468.pdf). The model (which can be found [here](https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering)) is implemented in Keras with a Tensorflow backend. \n",
    "<br><br> \n",
    "We begin with all necessary import statements. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/jennifervilla/Documents/tensorflow_python3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Flatten, Embedding, Merge, Input, Multiply\n",
    "from keras.layers.merge import Multiply\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Embedding\n",
    "Embeddings for the input image are taken from the last hidden layer of VGG19, which is a 4096 dimensional vector. Rather than run the images through VGG19 layers repeatedly, the authors of this network saved the embeddings for the images and use those as inputs to their network, rather than the raw images themselves. This is useful because this reduces computational intensity of the network, but it means that when we decide to change our CNN embedding, we will have to go back to using the raw images as input. \n",
    "<br><br>The 4096 element image embedding is then fed to a fully connected layer with 1024 output neurons and a tanh activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_model(dropout_rate):\n",
    "    print(\"Creating image model...\")\n",
    "    model = Sequential()\n",
    "    ##Feed the 4096 element image embedding through fully connected layer with 1024 output neurons and tanh activation\n",
    "    model.add(Dense(1024, input_dim=4096, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Using a previously trained GloVe embedding matrix, 300 element word2vec representations are created for each word in the question. The sequence of vectors is then fed to an LSTM with 2 hidden layers (with dropout applied). The output of this LSTM is then connected to a dense layer with 1024 output nerons and a tanh activation function. Because the embedding layer is instantiated with the trainable parameter set to false, the GloVe embedding matrix weights are not adjusted during training. \n",
    "<br> **Note:** The VQA paper says that it concatenates the last cell state and the last hidden state from both LSTM layers to form a 2048-dim embedding for the question, which is then fed to the 1024 unit FC layer. However, it is not clear from the code below that such a concatenation is being done. It looks like exclusively the last hidden state output from the 2nd hidden layer is being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate):\n",
    "    print(\"Creating text model...\")\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, embedding_dim, \n",
    "        weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
    "    model.add(LSTM(units=512, return_sequences=True, input_shape=(seq_length, embedding_dim)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units=512, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1024, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it together -- Combining Image and Word Embeddings\n",
    "Having created two 1024 dimensional embeddings, one for the image and one for the question, the model then merges these two. This merging is done via elementwise multiplication. \n",
    "The resulting 1024-element vector is then fed to a fully connected layer with 1000 output neurons and a tanh activation function. From there, it is fed to another fully connected layer with \"num_classes\" output neurons. \"num_classes\" represents the number of answers possible for the questions; each neuron maps to one answer. A softmax activation is used to reflect the fact that the final output is a probability vector whose elements sum to 1. The numerical value at any particular element in the output vector represents the probability that answer is the correct one for a particular image-question pair. The model's answer is the the answer with the max probability in the output vector. \n",
    "#### Loss function and training\n",
    "The model uses \"categorical_crossentropy\" as its loss function. This corresponds to the cross entropy metric defined in class; if $\\hat{y}$ is the softmax output reflecting probabilities weightings across all Z possible answers, then $y$ is a Z dimensions vector with a '1' at the position of the ground truth answer and '0' in all other positions. Given our specific dataset, the '1' is at the position of the most frequent answer given by the 10 human respondents.\n",
    "\n",
    "The model uses RMSprop as its optimization algorithm, with default hyperparameters (this is suggested in Keras documentation). Learning rate is 0.001, $\\rho$ is 0.9 (this was $\\beta$ in our lecture videos; the weighting of the current gradient relative to the historical average), $\\epsilon=1*10^{-8}$ (this is the 'fuzz' factor to protect against divide by zero errors), and learning rate decay is 0. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    vgg_model = img_model(dropout_rate)\n",
    "    lstm_model = Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate)\n",
    "    print(\"Merging final model...\")\n",
    "    fc_model = Sequential()\n",
    "    fc_model.add(Merge([vgg_model, lstm_model], mode='mul')) #Merge type layer now deprecated\n",
    "    fc_model.add(Dropout(dropout_rate))\n",
    "    fc_model.add(Dense(1000, activation='tanh'))\n",
    "    fc_model.add(Dropout(dropout_rate))\n",
    "    fc_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    #Setup loss function and defining training algorithm\n",
    "    fc_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return fc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "To test our model, we ran one training iteration of the model. Again, we begin with the necessary import statements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import argparse\n",
    "from prepare_data import *\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the get model function which creates the model using our previously defined vqa_model function. Before that, it prepares the embedding matrix which will be used to generate the word2vec representations of each of the question words. This function also checks for previously saved weights and loads them if found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dropout_rate, model_weights_filename):\n",
    "    print(\"Creating Model...\")\n",
    "    metadata = get_metadata()\n",
    "    num_classes = len(metadata['ix_to_ans'].keys())\n",
    "    num_words = len(metadata['ix_to_word'].keys())\n",
    "\n",
    "    embedding_matrix = prepare_embeddings(num_words, embedding_dim, metadata)\n",
    "    model = vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes)\n",
    "    if os.path.exists(model_weights_filename):\n",
    "        print(\"Loading Weights...\")\n",
    "        model.load_weights(model_weights_filename)\n",
    "    else:\n",
    "        print(\"No weights found at \" + model_weights_filename)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define functions for training and validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, batch_size, data_limit):\n",
    "    dropout_rate = 0.5\n",
    "    train_X, train_y = read_data(data_limit)    \n",
    "    model = get_model(dropout_rate, model_weights_filename)\n",
    "    checkpointer = ModelCheckpoint(filepath=ckpt_model_weights_filename, verbose=1, monitor='loss', save_best_only=True)\n",
    "    model.fit(train_X, train_y, epochs=epoch, batch_size=batch_size, callbacks=[checkpointer], shuffle=\"batch\")\n",
    "    print(\"Training Complete!\")\n",
    "    model.save_weights(ckpt_model_weights_filename + \".final\", overwrite=True)\n",
    "    \n",
    "def val():\n",
    "    val_X, val_y, multi_val_y = get_val_data() \n",
    "    model = get_model(0.0, model_weights_filename)\n",
    "    print(\"Evaluating Accuracy on validation set:\")\n",
    "    metric_vals = model.evaluate(val_X, val_y)\n",
    "    print(\"\")\n",
    "    for metric_name, metric_val in zip(model.metrics_names, metric_vals):\n",
    "        print(str(metric_name) + \" is \" + str(metric_val))\n",
    "\n",
    "    # Comparing prediction against multiple choice answers\n",
    "    true_positive = 0\n",
    "    preds = model.predict(val_X)\n",
    "    pred_classes = [np.argmax(_) for _ in preds]\n",
    "    for i, _ in enumerate(pred_classes):\n",
    "        if _ in multi_val_y[i]:\n",
    "            true_positive += 1\n",
    "    print(\"True positive rate: \" +  str(np.float(true_positive)/len(pred_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a test of the validation function to see how well our model does with the weights provided by the model authors. After running the line of code below to test the model on the entire validation set, the following results were obtained:\n",
    "<br><br>\n",
    "**Loss:** 2.76330976921 <br>\n",
    "**Accuracy:** 0.460777536375\n",
    "<br><br>\n",
    "This is approximately equivalent to the 45.03% validation accuracy reported in the original author's documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test the training function. For a quick test, lets only run 1 epoch with a batch size of 10, and only use 10 question-image pairs (instead of the full training set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epoch=3, batch_size=10, data_limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrating to a Functional-Style Keras Model\n",
    "### Merge Layer Deprecation\n",
    "When we ran the initial model, we saw warnings that as of Keras 2.0, the Merge layer used in `vqa_model()` has been deprecated. The suggested fix would be to instead use a Multiply layer to perform the elementwise multiplication, but the Multiply layer will only accept tensors. Several workarounds were tried, including using the new Multiply layer with \"vgg_model.output\" and \"lstm_model.output\", but we ran into issues there because we were using \"symbolic tensors\" rather than actual tensors. \n",
    "\n",
    "The proper approach, recommended by the Keras development community, is to switch from sequential style models, heavily utilized in Keras 1.0, to functional models, recommanded de-facto for Keras 2.0. Hence, we have redefined our models in functional Keras syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_model_func(dropout_rate):\n",
    "    print(\"Creating functional image model...\")\n",
    "    input_img = Input((4096,))\n",
    "    img_embedding = Dense(1024, input_dim=4096, activation='tanh')(input_img)\n",
    "    return input_img, img_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word2VecModel_func(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate):\n",
    "    print(\"Creating functional text model...\")\n",
    "    input_q = Input((seq_length,))\n",
    "    x = Embedding(num_words, embedding_dim, weights=[embedding_matrix], trainable=False)(input_q)\n",
    "    x = LSTM(units=512, return_sequences=True)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = LSTM(units=512, return_sequences=False)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    q_embedding = Dense(1024, activation='tanh')(x)\n",
    "    return input_q, q_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_model_func(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    input_img, img_embedding = img_model(dropout_rate)\n",
    "    input_q, q_embedding = Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate)\n",
    "    print(\"Merging final model...\")\n",
    "    combined = Multiply()([img_embedding, q_embedding])\n",
    "    combined = Dropout(dropout_rate)(combined)\n",
    "    combined = Dense(1000, activation='tanh')(combined)\n",
    "    combined = Dropout(dropout_rate)(combined)\n",
    "    predictions = Dense(num_classes, activation='softmax')(combined)\n",
    "    fc_model = Model(inputs=[input_img, input_q], outputs=predictions)\n",
    "    fc_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return fc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactoring to functional style models breaks the ability to load the pre-trained weights, trained on the sequential model definition.\n",
    "\n",
    "Therefore, to validate our functional model implementation, we trained the model from scratch overnight, and recorded evaluation results against the validation set at every 10 epochs. We wrote a function `loop(args)` to print out a heartbeat as the training continued.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(args):\n",
    "    for i in range(1, args.num_loops + 1):\n",
    "        model = train(args)\n",
    "        if args.save_all:\n",
    "            model.save_weights(model_weights_filename+\"_epoch_\"+str(i*args.epoch), overwrite=False)\n",
    "        metrics, true_positive_rate = val()\n",
    "        with open(\"training_log\", \"a\") as val_log:\n",
    "            val_log.write(\"After training epoch \" + str(args.epoch * i)+\"\\n\")\n",
    "            for name, value in metrics:\n",
    "                val_log.write(name + \" \" + str(value)+\"\\n\")\n",
    "            val_log.write(\"True_positive_rate: \" + str(true_positive_rate)+\"\\n\")\n",
    "        print(\"Finished loop number: \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon running the `loop` function, our results were as follows: <br>\n",
    "\n",
    "|  Epoch        | Accuracy            | True Postitve Rate   | Loss\n",
    "| :-------------: |:-------------| :-----|:-----|\n",
    "| 10     | 0.445552702614 | 0.573844558562 |2.53969596104|\n",
    "| 20      | 0.454802817829      |   0.58548949898 |2.59171895231|\n",
    "| 30 |         0.45152742116|0.582181183751  |2.6666429342|\n",
    "|40|0.458144051616|  0.587061360195 |2.76133057947\n",
    "|50 |0.453848179604 |0.581045493449| 2.9734828312\n",
    "|60 |0.449074988479 |0.577926459938 |3.08288818925\n",
    "|70 |0.450976035289 |0.577959378498 |3.19416170537\n",
    "|80 |0.449881493186 |0.577910000658 |3.32517784668\n",
    "|90 | 0.445272894858| 0.572231549147 |3.3803069139\n",
    "|100 |0.44325663309 |0.571375666601|3.44215128993\n",
    "\n",
    "Here, **Accuracy** measures the rate at which the predicted answer is the same as the top human answer.\n",
    "\n",
    "**True Positive Rate** measures the rate at which the predicted answer matches any one of the 10 human provided answers.\n",
    "\n",
    "**Loss** measures the categorical cross-entropy on the validation set.\n",
    "\n",
    "After training 30 epochs, our functional model achieved an accuracy on the validation set of 45.15%, which is almost identical to the 45.03% accuracy figure reported by the original implementers of the sequential model. We believe the differences between our accuracies are caused by using difference batch_sizes. \n",
    "\n",
    "Our model achieved highest accuracy after 40 epochs, with an accuracy of 45.81%. After 40 epochs, both the accuracy and true positive rate declines, indicating overfitting of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Next Steps\n",
    "1. **Test different image embeddings**: Currently, our model uses image embeddings taken from the last hidden layer of VGG19. We would like to test the performance of embeddings taken from other well-known CNN models. Keras has [open source implementations](https://github.com/fchollet/keras/tree/master/keras/applications) of Resnet-50, Inception-Resnet_v2, Inception_v3, and Xception. A search of Github also reveals a Keras [Resnet-152](https://gist.github.com/flyyufelix/7e2eafb149f72f4d38dd661882c554a6) implementation, which might also be worth trying since this was the [model](https://web.stanford.edu/class/cs224n/reports/2748290.pdf) used by another Stanford team. As with our existing model, it probably makes sense to pre-compute the image features by running the VQA dataset images through the chosen network for embedding. Using our new set of image features, we can re-train our model to see if we can achieve better accuracy. \n",
    "2. **Implement New Cost Function for Soft Cross Entropy Loss**: Currently our model uses binary cross-entropy loss, with the ground-truth answer being a one-hot vector encoding the answer given by the majority of the human respondents polled. However, the VQA metric actually awards partial credit to models that output answers that match any of the ten human respondent answers. Thus, there is currently a disconnect between the loss function and the evaluation metric. One of the top performers in this year's VQA challenge sought to address this by proposing a [soft cross entropy loss](https://ilija139.github.io/pub/cvpr2017_vqa.pdf) function. This function calculates a weighted average of all unique ground truth answers given by the 10 human respondents. In the paper, these researchers achieved ~1.2-1.6% improved accuracy across a variety of model architectures. We would like to see if this improvement translates to our model as well. \n",
    "3. **Implement Multi-Modal Factorized Bilinear Pooling**: The [second place winner](https://arxiv.org/pdf/1708.01471.pdf) in this year’s VQA challenge proposed the concept of multi-modal factorized bilinear pooling. The idea is that using concatenation or elementwise multiplication to combine image and question embeddings (each of which represents information from a different modality, hence the term “multi-modal”) may limit model performance. A more sophisticated approach to fusing these two might be necessary. Thus they proposed “multi-modal factorized bilinear pooling”, which amounts to a combination of element-wise multiplication, fully connected layers, and sum pooling. We would like to see the impact of substituting this technique in place of our current “Multiply” layer in our VQA model. Once we implement co-attention (see $4$), we could also incorporate this technique into that model. (Authors of the original paper saw improvement in both simpler model architectures, such as the one we currently have, and those with co-attention).\n",
    "4. **Implement Image-Question Co-attention**: All of the top performing VQA models use some form of attention. While early research focused exclusively on image attention [[1]](https://arxiv.org/abs/1511.07394), more recent work has combined image attention with question attention [[2]](https://arxiv.org/abs/1606.00061) [[3]](https://web.stanford.edu/class/cs224n/reports/2748290.pdf). The philosophy behind the latter approach is just as certain regions of the image are more relevant than others, certain words in the question are more helpful in answering than others. A variety of different co-attention architectures have been proposed, including hierarchical co-attention, which looks at question attention recursively on word, phrase, and full question levels. Other co-attention models (like [3]) include information indicating the part of speech of a word. We plan on starting with an architecture like the one proposed in the \"Multi-Modal Factorized Bilinear Pooling\" paper, which won second place [[4]](https://arxiv.org/pdf/1708.01471.pdf). The experimenters provide their [source code](https://github.com/yuzcccc/vqa-mfb), but it is written in Caffe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
